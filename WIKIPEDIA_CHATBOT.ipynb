{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMqDkohL3blRiFUYkkbXI2Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install langchain-google-genai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DZ1r4hIS31qi","executionInfo":{"status":"ok","timestamp":1743653931100,"user_tz":-330,"elapsed":6576,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"93ed8682-9266-4e85-e006-e7a8536de7f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-google-genai\n","  Downloading langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n","Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Collecting google-ai-generativelanguage<0.7.0,>=0.6.16 (from langchain-google-genai)\n","  Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl.metadata (9.8 kB)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.49)\n","Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.24.2)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.38.0)\n","Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.26.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.29.4)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.3.19)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (6.0.2)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (4.13.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.69.2)\n","Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.32.3)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (1.71.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (4.9)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.23.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (0.14.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain-google-genai) (2.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain-google-genai) (1.3.1)\n","Downloading langchain_google_genai-2.1.2-py3-none-any.whl (42 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading google_ai_generativelanguage-0.6.17-py3-none-any.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n","  Attempting uninstall: google-ai-generativelanguage\n","    Found existing installation: google-ai-generativelanguage 0.6.15\n","    Uninstalling google-ai-generativelanguage-0.6.15:\n","      Successfully uninstalled google-ai-generativelanguage-0.6.15\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.17 langchain-google-genai-2.1.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"af5d45e1315542d6808a2d2e4925c047"}},"metadata":{}}]},{"cell_type":"code","source":["!pip install langchain\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D9gmoO8C46YK","executionInfo":{"status":"ok","timestamp":1743654289821,"user_tz":-330,"elapsed":3958,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"5d8b1e98-c964-46ef-9acc-41097becf3b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.19)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.0)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.13.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n"]}]},{"cell_type":"code","source":["!pip install faiss-cpu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VIE6GqNs5Tpy","executionInfo":{"status":"ok","timestamp":1743654314264,"user_tz":-330,"elapsed":6465,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"6009d954-79f0-4ef8-c07a-c735b6cd18b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.10.0\n"]}]},{"cell_type":"code","source":["!pip install wikipedia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKW72xV_5Z5p","executionInfo":{"status":"ok","timestamp":1743654337672,"user_tz":-330,"elapsed":5557,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"467df908-ca43-4959-e7f1-6f2465f26ad3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wikipedia\n","  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.3)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.13.0)\n","Building wheels for collected packages: wikipedia\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=3d880cd317bc9f9201c748ca7a1ec509f2064dd676227f09be688a3996462a29\n","  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n","Successfully built wikipedia\n","Installing collected packages: wikipedia\n","Successfully installed wikipedia-1.4.0\n"]}]},{"cell_type":"code","source":["!pip install langchain\n","!pip install faiss-cpu\n","!pip install langchain_google_genai\n","!pip install wikipedia"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSuwam6y6ALi","executionInfo":{"status":"ok","timestamp":1743654506075,"user_tz":-330,"elapsed":16936,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"03852bd3-8852-4a78-911e-b7bdeead969a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.19)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.0)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.13.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.11/dist-packages (2.1.2)\n","Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (1.2.0)\n","Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.16 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.6.17)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (0.3.49)\n","Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_google_genai) (2.11.0)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.24.2)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.38.0)\n","Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.26.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.29.4)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (0.3.19)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (9.0.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (1.33)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (6.0.2)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (4.13.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain_google_genai) (0.4.0)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.69.2)\n","Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.32.3)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (1.71.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (4.9)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (3.0.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (0.23.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (4.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (1.0.7)\n","Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (3.10)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (0.14.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (0.6.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.16->langchain_google_genai) (2.3.0)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.49->langchain_google_genai) (1.3.1)\n","Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.3)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.13.0)\n"]}]},{"cell_type":"code","source":["pip install -U langchain-community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkB6FqDw6vhB","executionInfo":{"status":"ok","timestamp":1743654689840,"user_tz":-330,"elapsed":7406,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"6f2c4321-67a4-4150-f945-a6a189ac4f7a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain-community\n","  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.49)\n","Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.19)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.11.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.13.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.33.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.4.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n","Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n","Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["import google.generativeai as genai\n","import os\n","\n","# Replace with your actual API key\n","GOOGLE_API_KEY = \"\"\n","\n","genai.configure(api_key=GOOGLE_API_KEY)\n","\n","models = genai.list_models()\n","\n","for model in models:\n","    print(f\"Model: {model.name}\")\n","    print(f\"  Description: {model.description}\")\n","    print(f\"  Supported generation methods: {model.supported_generation_methods}\")\n","    print(\"-\" * 20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AJGALlvm76NA","executionInfo":{"status":"ok","timestamp":1743655146819,"user_tz":-330,"elapsed":3032,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"7c0f3ff1-d4e3-4523-db23-a6d52c889b20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: models/chat-bison-001\n","  Description: A legacy text-only model optimized for chat conversations\n","  Supported generation methods: ['generateMessage', 'countMessageTokens']\n","--------------------\n","Model: models/text-bison-001\n","  Description: A legacy model that understands text and generates text as an output\n","  Supported generation methods: ['generateText', 'countTextTokens', 'createTunedTextModel']\n","--------------------\n","Model: models/embedding-gecko-001\n","  Description: Obtain a distributed representation of a text.\n","  Supported generation methods: ['embedText', 'countTextTokens']\n","--------------------\n","Model: models/gemini-1.0-pro-vision-latest\n","  Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-pro-vision\n","  Description: The original Gemini 1.0 Pro Vision model version which was optimized for image understanding. Gemini 1.0 Pro Vision was deprecated on July 12, 2024. Move to a newer Gemini version.\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-1.5-pro-latest\n","  Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-1.5-pro-001\n","  Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n","  Supported generation methods: ['generateContent', 'countTokens', 'createCachedContent']\n","--------------------\n","Model: models/gemini-1.5-pro-002\n","  Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in September of 2024.\n","  Supported generation methods: ['generateContent', 'countTokens', 'createCachedContent']\n","--------------------\n","Model: models/gemini-1.5-pro\n","  Description: Stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens, released in May of 2024.\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-1.5-flash-latest\n","  Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-1.5-flash-001\n","  Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n","  Supported generation methods: ['generateContent', 'countTokens', 'createCachedContent']\n","--------------------\n","Model: models/gemini-1.5-flash-001-tuning\n","  Description: Version of Gemini 1.5 Flash that supports tuning, our fast and versatile multimodal model for scaling across diverse tasks, released in May of 2024.\n","  Supported generation methods: ['generateContent', 'countTokens', 'createTunedModel']\n","--------------------\n","Model: models/gemini-1.5-flash\n","  Description: Alias that points to the most recent stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks.\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-1.5-flash-002\n","  Description: Stable version of Gemini 1.5 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in September of 2024.\n","  Supported generation methods: ['generateContent', 'countTokens', 'createCachedContent']\n","--------------------\n","Model: models/gemini-1.5-flash-8b\n","  Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n","  Supported generation methods: ['createCachedContent', 'generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-1.5-flash-8b-001\n","  Description: Stable version of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n","  Supported generation methods: ['createCachedContent', 'generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-1.5-flash-8b-latest\n","  Description: Alias that points to the most recent production (non-experimental) release of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model, released in October of 2024.\n","  Supported generation methods: ['createCachedContent', 'generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-1.5-flash-8b-exp-0827\n","  Description: Experimental release (August 27th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-1.5-flash-8b-exp-0924\n","  Description: Experimental release (September 24th, 2024) of Gemini 1.5 Flash-8B, our smallest and most cost effective Flash model. Replaced by Gemini-1.5-flash-8b-001 (stable).\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.5-pro-exp-03-25\n","  Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-flash-exp\n","  Description: Gemini 2.0 Flash Experimental\n","  Supported generation methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n","--------------------\n","Model: models/gemini-2.0-flash\n","  Description: Gemini 2.0 Flash\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-flash-001\n","  Description: Stable version of Gemini 2.0 Flash, our fast and versatile multimodal model for scaling across diverse tasks, released in January of 2025.\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-flash-exp-image-generation\n","  Description: Gemini 2.0 Flash (Image Generation) Experimental\n","  Supported generation methods: ['generateContent', 'countTokens', 'bidiGenerateContent']\n","--------------------\n","Model: models/gemini-2.0-flash-lite-001\n","  Description: Stable version of Gemini 2.0 Flash Lite\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-flash-lite\n","  Description: Gemini 2.0 Flash-Lite\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-flash-lite-preview-02-05\n","  Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-flash-lite-preview\n","  Description: Preview release (February 5th, 2025) of Gemini 2.0 Flash Lite\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-pro-exp\n","  Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-pro-exp-02-05\n","  Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-exp-1206\n","  Description: Experimental release (March 25th, 2025) of Gemini 2.5 Pro\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-flash-thinking-exp-01-21\n","  Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-flash-thinking-exp\n","  Description: Experimental release (January 21st, 2025) of Gemini 2.0 Flash Thinking\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemini-2.0-flash-thinking-exp-1219\n","  Description: Gemini 2.0 Flash Thinking Experimental\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/learnlm-1.5-pro-experimental\n","  Description: Alias that points to the most recent stable version of Gemini 1.5 Pro, our mid-size multimodal model that supports up to 2 million tokens.\n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemma-3-1b-it\n","  Description: \n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemma-3-4b-it\n","  Description: \n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemma-3-12b-it\n","  Description: \n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/gemma-3-27b-it\n","  Description: \n","  Supported generation methods: ['generateContent', 'countTokens']\n","--------------------\n","Model: models/embedding-001\n","  Description: Obtain a distributed representation of a text.\n","  Supported generation methods: ['embedContent']\n","--------------------\n","Model: models/text-embedding-004\n","  Description: Obtain a distributed representation of a text.\n","  Supported generation methods: ['embedContent']\n","--------------------\n","Model: models/gemini-embedding-exp-03-07\n","  Description: Obtain a distributed representation of a text.\n","  Supported generation methods: ['embedContent']\n","--------------------\n","Model: models/gemini-embedding-exp\n","  Description: Obtain a distributed representation of a text.\n","  Supported generation methods: ['embedContent']\n","--------------------\n","Model: models/aqa\n","  Description: Model trained to return answers to questions that are grounded in provided sources, along with estimating answerable probability.\n","  Supported generation methods: ['generateAnswer']\n","--------------------\n","Model: models/imagen-3.0-generate-002\n","  Description: Vertex served Imagen 3.0 002 model\n","  Supported generation methods: ['predict']\n","--------------------\n"]}]},{"cell_type":"markdown","source":["### DOMAIN SPECIFIC CHATBOT"],"metadata":{"id":"wbdboihPEciN"}},{"cell_type":"code","source":["import os\n","import langchain\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import FAISS\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain.chains import RetrievalQA\n","from langchain_community.document_loaders import WikipediaLoader\n","\n","# Directly set the API key in Colab (less secure, but simpler for Colab)\n","GOOGLE_API_KEY = \"\"  # Replace with your actual API key\n","\n","if not GOOGLE_API_KEY:\n","    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n","\n","def create_wikipedia_chatbot(topic):\n","    \"\"\"\n","    Creates a chatbot that retrieves information from Wikipedia and answers questions.\n","\n","    Args:\n","        topic (str): The Wikipedia page topic to retrieve information from.\n","\n","    Returns:\n","        RetrievalQA: A LangChain RetrievalQA chain, or None if an error occurred.\n","    \"\"\"\n","\n","    try:\n","        # Load data from Wikipedia\n","        loader = WikipediaLoader(query=topic, load_max_docs=2)  # Adjust load_max_docs as needed.\n","        documents = loader.load()\n","\n","        # Split the text into chunks\n","        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","        texts = text_splitter.split_documents(documents)\n","\n","        # Create embeddings using Google's Gemini embeddings\n","        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n","\n","        # Create a FAISS vector store\n","        db = FAISS.from_documents(texts, embeddings)\n","\n","        # Create a retriever from the vector store\n","        retriever = db.as_retriever()\n","\n","        # Initialize the ChatGoogleGenerativeAI model\n","        # Use gemini 1.5 pro since it is available in your list of models.\n","        llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n","\n","        # Create a RetrievalQA chain\n","        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n","\n","        return qa_chain\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None\n","\n","def ask_question(qa_chain, question):\n","    \"\"\"\n","    Asks a question to the chatbot and prints the answer.\n","\n","    Args:\n","        qa_chain (RetrievalQA): The LangChain RetrievalQA chain.\n","        question (str): The question to ask.\n","    \"\"\"\n","    if qa_chain:\n","        try:\n","            result = qa_chain({\"query\": question})\n","            print(\"Question:\", question)\n","            print(\"Answer:\", result[\"result\"])\n","            print(\"\\nSource Documents:\")\n","            for doc in result[\"source_documents\"]:\n","                print(f\"  - {doc.metadata['title']}\")\n","        except Exception as e:\n","            print(f\"Error asking question: {e}\")\n","    else:\n","        print(\"Chatbot not initialized.\")\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    topic = input(\"Enter a Wikipedia topic: \")  # Get topic from user\n","    qa_chain = create_wikipedia_chatbot(topic)\n","\n","    if qa_chain:\n","        while True:\n","            user_question = input(\"Ask a question (or type 'exit'): \")\n","            if user_question.lower() == \"exit\":\n","                break\n","            ask_question(qa_chain, user_question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KMHh_I7u-krv","executionInfo":{"status":"ok","timestamp":1743656682851,"user_tz":-330,"elapsed":339903,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"d3fa7c08-4de2-4681-a748-1ef73cb33871"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter a Wikipedia topic: Physics\n","Ask a question (or type 'exit'): What is relativity\n","Question: What is relativity\n","Answer: This text discusses aspects of physics, including particle physics and mentions Aristotle's contributions to the field.  It does *not* contain any information about relativity. Therefore, I cannot answer your question using the provided context.\n","\n","Source Documents:\n","  - Physics\n","  - Physics\n","  - Particle physics\n","  - Particle physics\n","Ask a question (or type 'exit'): what is motionh\n","Question: what is motionh\n","Answer: It seems you may have misspelled \"motion\".  Motion is the change in position of an object over time.  It's a fundamental concept in physics.\n","\n","Source Documents:\n","  - Particle physics\n","  - Physics\n","  - Physics\n","  - Particle physics\n","Ask a question (or type 'exit'): what is ethar medium\n","Question: what is ethar medium\n","Answer: This document doesn't contain the answer to the question about the \"ethar medium\".  It discusses fundamental particles, forces, and the standard model of particle physics, but it does not mention anything called \"ethar medium\".\n","\n","Source Documents:\n","  - Particle physics\n","  - Particle physics\n","  - Particle physics\n","  - Particle physics\n","Ask a question (or type 'exit'): exit\n"]}]},{"cell_type":"markdown","source":["### GENERALIZED BOT"],"metadata":{"id":"Knz9pU6oEhGz"}},{"cell_type":"code","source":["import os\n","import langchain\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain.chains import RetrievalQA\n","from langchain_community.document_loaders import WikipediaLoader\n","import wikipedia\n","\n","# Directly set the API key in Colab (less secure, but simpler for Colab)\n","GOOGLE_API_KEY = \"\"  # Replace with your actual API key\n","\n","if not GOOGLE_API_KEY:\n","    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n","\n","def create_wikipedia_chatbot(question):\n","    \"\"\"\n","    Creates a chatbot that retrieves information from Wikipedia based on the question.\n","\n","    Args:\n","        question (str): The user's question.\n","\n","    Returns:\n","        RetrievalQA: A LangChain RetrievalQA chain, or None if an error occurred.\n","    \"\"\"\n","\n","    try:\n","        # Use the question itself as the search query\n","        try:\n","            # Get the closest wikipedia page title.\n","            best_guess = wikipedia.search(question)[0]\n","            loader = WikipediaLoader(query=best_guess, load_max_docs=2)\n","            documents = loader.load()\n","        except Exception as e:\n","            print(f\"Error loading Wikipedia data: {e}\")\n","            return None\n","\n","        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","        texts = text_splitter.split_documents(documents)\n","\n","        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n","        db = FAISS.from_documents(texts, embeddings)\n","        retriever = db.as_retriever()\n","        llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n","        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n","\n","        return qa_chain\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None\n","\n","def ask_question(question):\n","    \"\"\"\n","    Asks a question to the chatbot and prints the answer.\n","\n","    Args:\n","        question (str): The question to ask.\n","    \"\"\"\n","    qa_chain = create_wikipedia_chatbot(question)\n","    if qa_chain:\n","        try:\n","            result = qa_chain({\"query\": question})\n","            print(\"Question:\", question)\n","            print(\"Answer:\", result[\"result\"])\n","            print(\"\\nSource Documents:\")\n","            for doc in result[\"source_documents\"]:\n","                print(f\"  - {doc.metadata['title']}\")\n","        except Exception as e:\n","            print(f\"Error asking question: {e}\")\n","    else:\n","        print(\"Chatbot not initialized.\")\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    while True:\n","        user_question = input(\"Ask a question (or type 'exit'): \")\n","        if user_question.lower() == \"exit\":\n","            break\n","        ask_question(user_question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vn3SZ92qDd22","executionInfo":{"status":"ok","timestamp":1743657217039,"user_tz":-330,"elapsed":146741,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"de0bbe51-fb89-4f3e-c949-2b74f39773ee"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Ask a question (or type 'exit'): what is chatbot\n","Question: what is chatbot\n","Answer: A chatbot (originally chatterbot) is a software application or web interface designed to have textual or spoken conversations.  Modern chatbots often leverage generative AI and natural language processing to simulate human-like conversation, but simpler versions based on pattern-matching have existed for decades.  People often prefer interacting with human-like programs, making chatbots useful for various interactive systems, especially customer service and support.  Recent advances in AI and large language models have led to a surge in chatbot development and popularity, with examples like ChatGPT, Copilot, DeepSeek, and Gemini.\n","\n","Source Documents:\n","  - Chatbot\n","  - Chatbot\n","  - Chatbot\n","  - Grok (chatbot)\n","Ask a question (or type 'exit'): what is relativity in physics\n","Question: what is relativity in physics\n","Answer: Relativity in physics usually refers to two interrelated theories by Albert Einstein: special relativity and general relativity.  Special relativity, published in 1905, applies to all physical phenomena in the absence of gravity.  General relativity, published in 1915, explains the law of gravitation and its relation to other forces of nature. It applies to the cosmological and astrophysical realm.\n","\n","Source Documents:\n","  - Theory of relativity\n","  - Theory of relativity\n","  - Theory of relativity\n","  - General relativity\n","Ask a question (or type 'exit'): exit\n"]}]},{"cell_type":"markdown","source":["### UPLOADING DOCUMENT AND ASKING QUESTIONS\n","\n"],"metadata":{"id":"VSbsmfm_Gpy7"}},{"cell_type":"code","source":["!pip install pypdf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w1hQvvgtKTfJ","executionInfo":{"status":"ok","timestamp":1743658780540,"user_tz":-330,"elapsed":3328,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"0e88268a-bb57-40f9-a227-75d176c53bf9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pypdf\n","  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n","Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/302.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/302.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","Successfully installed pypdf-5.4.0\n"]}]},{"cell_type":"code","source":["import os\n","import langchain\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain.chains import RetrievalQA\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","# Directly set the API key in Colab (less secure, but simpler for Colab)\n","GOOGLE_API_KEY = \"\"  # Replace with your actual API key\n","\n","if not GOOGLE_API_KEY:\n","    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n","\n","def create_pdf_chatbot(pdf_path, question):\n","    \"\"\"\n","    Creates a chatbot that retrieves information from a PDF document based on the question.\n","\n","    Args:\n","        pdf_path (str): The path to the PDF document.\n","        question (str): The user's question.\n","\n","    Returns:\n","        RetrievalQA: A LangChain RetrievalQA chain, or None if an error occurred.\n","    \"\"\"\n","\n","    try:\n","        loader = PyPDFLoader(pdf_path)\n","        documents = loader.load()\n","\n","        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","        texts = text_splitter.split_documents(documents)\n","\n","        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n","        db = FAISS.from_documents(texts, embeddings)\n","        retriever = db.as_retriever()\n","        llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n","        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n","\n","        return qa_chain\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None\n","\n","def ask_question_pdf(pdf_path, question):\n","    \"\"\"\n","    Asks a question to the chatbot based on a PDF document and prints the answer.\n","\n","    Args:\n","        pdf_path (str): The path to the PDF document.\n","        question (str): The question to ask.\n","    \"\"\"\n","    qa_chain = create_pdf_chatbot(pdf_path, question)\n","    if qa_chain:\n","        try:\n","            result = qa_chain({\"query\": question})\n","            print(\"Question:\", question)\n","            print(\"Answer:\", result[\"result\"])\n","            print(\"\\nSource Documents:\")\n","            for doc in result[\"source_documents\"]:\n","                print(f\"  - Page {doc.metadata['page']}\")\n","        except Exception as e:\n","            print(f\"Error asking question: {e}\")\n","    else:\n","        print(\"Chatbot not initialized.\")\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    pdf_file_path =input(\"enter:\")  # Replace with the path to your PDF file.\n","    if not os.path.exists(pdf_file_path):\n","        print(f\"Error: PDF file '{pdf_file_path}' not found.\")\n","    else:\n","\n","        while True:\n","            user_question = input(\"Ask a question (or type 'exit'): \")\n","            if user_question.lower() == \"exit\":\n","                break\n","            ask_question_pdf(pdf_file_path, user_question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDnuOZYHD23c","executionInfo":{"status":"ok","timestamp":1743659638601,"user_tz":-330,"elapsed":853565,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"e0a8b7c2-9971-4e1c-8128-b0a4fbdd82b1"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["enter:Autonomous_vehicles_The_future_of_automobiles.pdf\n","Ask a question (or type 'exit'): What are the keywords \n","Question: What are the keywords \n","Answer: Steering, speed, motion planning, navigation, localization, GPS, RADAR, LIDAR, INS.\n","\n","Source Documents:\n","  - Page 2\n","  - Page 5\n","  - Page 5\n","  - Page 2\n","Ask a question (or type 'exit'): Give the most influential autonomous car company\n","Question: Give the most influential autonomous car company\n","Answer: This document mentions a Figure 1 that lists the most influential autonomous car companies, but the figure itself is not included.  Therefore, I cannot answer your question.\n","\n","Source Documents:\n","  - Page 0\n","  - Page 5\n","  - Page 5\n","  - Page 1\n","Ask a question (or type 'exit'): how autonomous vehicles are classified\n","Question: how autonomous vehicles are classified\n","Answer: The National Highway Traffic Safety Administration (NHTSA) classifies autonomous vehicles into four levels:\n","\n","* **Level 1: Function Specific Automation.** This level includes automation of specific control functions, such as cruise control, lane guidance, and automated parallel parking.  Drivers are fully engaged and responsible for overall vehicle control.\n","* **Level 2: Combined Function Automation.** This level signifies automation of multiple integrated control functions, such as adaptive cruise control with lane centering. Drivers are still responsible for monitoring the roadway and must be ready to take control at any time.  \n","\n","Source Documents:\n","  - Page 1\n","  - Page 0\n","  - Page 3\n","  - Page 5\n","Ask a question (or type 'exit'): exit\n"]}]},{"cell_type":"markdown","source":["### HEALTHCARE AI BOT"],"metadata":{"id":"_j37CRQoQpfF"}},{"cell_type":"code","source":["import os\n","import langchain\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain.chains import RetrievalQA\n","from langchain_community.document_loaders import PyPDFLoader\n","\n","# Directly set the API key in Colab (less secure, but simpler for Colab)\n","GOOGLE_API_KEY = \"\"  # Replace with your actual API key\n","\n","if not GOOGLE_API_KEY:\n","    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n","\n","def create_pdf_chatbot(pdf_path, question):\n","    \"\"\"\n","    Creates a chatbot that retrieves information from a PDF document based on the question.\n","\n","    Args:\n","        pdf_path (str): The path to the PDF document.\n","        question (str): The user's question.\n","\n","    Returns:\n","        RetrievalQA: A LangChain RetrievalQA chain, or None if an error occurred.\n","    \"\"\"\n","\n","    try:\n","        loader = PyPDFLoader(pdf_path)\n","        documents = loader.load()\n","\n","        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","        texts = text_splitter.split_documents(documents)\n","\n","        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n","        db = FAISS.from_documents(texts, embeddings)\n","        retriever = db.as_retriever()\n","        llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n","        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n","\n","        return qa_chain\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None\n","\n","def ask_question_pdf(pdf_path, question):\n","    \"\"\"\n","    Asks a question to the chatbot based on a PDF document and prints the answer.\n","\n","    Args:\n","        pdf_path (str): The path to the PDF document.\n","        question (str): The question to ask.\n","    \"\"\"\n","    qa_chain = create_pdf_chatbot(pdf_path, question)\n","    if qa_chain:\n","        try:\n","            result = qa_chain({\"query\": question})\n","            print(\"Question:\", question)\n","            print(\"Answer:\", result[\"result\"])\n","            print(\"\\nSource Documents:\")\n","            for doc in result[\"source_documents\"]:\n","                print(f\"  - Page {doc.metadata['page']}\")\n","        except Exception as e:\n","            print(f\"Error asking question: {e}\")\n","    else:\n","        print(\"Chatbot not initialized.\")\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    pdf_file_path =input(\"enter:\")  # Replace with the path to your PDF file.\n","    if not os.path.exists(pdf_file_path):\n","        print(f\"Error: PDF file '{pdf_file_path}' not found.\")\n","    else:\n","\n","        while True:\n","            user_question = input(\"Ask a question (or type 'exit'): \")\n","            if user_question.lower() == \"exit\":\n","                break\n","            ask_question_pdf(pdf_file_path, user_question)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_mOwyesvG3Xy","executionInfo":{"status":"ok","timestamp":1743660600908,"user_tz":-330,"elapsed":446196,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"d18253bf-273e-436a-c655-3c145e45651d"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["enter:parkinsons_htr_english_20-ns-139_508c.pdf\n","Ask a question (or type 'exit'): what is parkinsons disease\n","Question: what is parkinsons disease\n","Answer: Parkinson's disease (PD) is a progressive nervous system disorder affecting movement.  It develops gradually, often starting with a barely noticeable tremor in one hand. While tremor is a common symptom, the disorder also commonly causes stiffness or slowing of movement.\n","\n","Source Documents:\n","  - Page 4\n","  - Page 2\n","  - Page 15\n","  - Page 9\n","Ask a question (or type 'exit'): what is the cause\n","Question: what is the cause\n","Answer: The precise cause of Parkinson's Disease is unknown. While some cases are linked to specific genetic mutations, most are not. It's believed that the disease likely results from a combination of genetic and environmental factors.\n","\n","Source Documents:\n","  - Page 10\n","  - Page 2\n","  - Page 0\n","  - Page 4\n","Ask a question (or type 'exit'): is there any remedy\n","Question: is there any remedy\n","Answer: The text mentions that presently, there is no cure for Parkinson's Disease. However, medications and surgery may improve motor symptoms.  It also suggests complementary and supportive therapies are available.  More information can be found in the sections on \"Drug Therapy,\" \"Surgery,\" and \"Complementary and Supportive Therapies.\"\n","\n","Source Documents:\n","  - Page 19\n","  - Page 2\n","  - Page 21\n","  - Page 18\n","Ask a question (or type 'exit'): EXIT\n"]}]},{"cell_type":"code","source":["pip install xmltodict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4NZL4JnR9jV","executionInfo":{"status":"ok","timestamp":1743660787012,"user_tz":-330,"elapsed":3931,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"a2d0a51f-cbb8-49b8-d0aa-3db6ccff8aae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting xmltodict\n","  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n","Downloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n","Installing collected packages: xmltodict\n","Successfully installed xmltodict-0.14.2\n"]}]},{"cell_type":"code","source":["import os\n","import langchain\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain.chains import RetrievalQA\n","from langchain_community.document_loaders import PubMedLoader  # Changed to PubMedLoader\n","\n","# Directly set the API key in Colab (less secure, but simpler for Colab)\n","GOOGLE_API_KEY = \"\"  # Replace with your actual API key\n","\n","if not GOOGLE_API_KEY:\n","    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n","\n","def create_pubmed_chatbot(query):  # Changed to accept a query instead of pdf_path\n","    \"\"\"\n","    Creates a chatbot that retrieves information from PubMed based on the question.\n","\n","    Args:\n","        query (str): The search query for PubMed.\n","\n","    Returns:\n","        RetrievalQA: A LangChain RetrievalQA chain, or None if an error occurred.\n","    \"\"\"\n","\n","    try:\n","        loader = PubMedLoader(query=query, load_max_docs=5)  # Adjusted load_max_docs\n","        documents = loader.load()\n","\n","        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","        texts = text_splitter.split_documents(documents)\n","\n","        embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n","        db = FAISS.from_documents(texts, embeddings)\n","        retriever = db.as_retriever()\n","        llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n","        qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n","\n","        return qa_chain\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        return None\n","\n","def ask_question_pubmed(query):  # Changed to accept a query.\n","    \"\"\"\n","    Asks a question to the chatbot based on PubMed and prints the answer.\n","\n","    Args:\n","        query (str): The search query for PubMed.\n","    \"\"\"\n","    qa_chain = create_pubmed_chatbot(query)\n","    if qa_chain:\n","        try:\n","            result = qa_chain({\"query\": query})\n","            print(\"Question:\", query)\n","            print(\"Answer:\", result[\"result\"])\n","          # changed from page to title\n","        except Exception as e:\n","            print(f\"Error asking question: {e}\")\n","    else:\n","        print(\"Chatbot not initialized.\")\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    while True:\n","        user_question = input(\"Enter your health-related question (or type 'exit'): \")\n","        if user_question.lower() == \"exit\":\n","            break\n","        ask_question_pubmed(user_question) # changed to user question."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxV9B9jkPn5d","executionInfo":{"status":"ok","timestamp":1743660921659,"user_tz":-330,"elapsed":132899,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"70d621e6-a517-4194-ecd5-b086f4a3324f"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your health-related question (or type 'exit'): what is pneumonia\n","Too Many Requests, waiting for 0.20 seconds...\n","Question: what is pneumonia\n","Answer: This text does not provide a definition of pneumonia, but it does mention pneumonia as a complication studied in trauma patients (incidence of 19%). It also mentions lower respiratory infections (LRIs), noting that Streptococcus pneumoniae is the leading cause of mortality from LRIs.  This suggests a link between pneumonia and LRIs, but doesn't define pneumonia itself.\n","Enter your health-related question (or type 'exit'): what is streptococcus pneumoniae\n","Too Many Requests, waiting for 0.20 seconds...\n","Question: what is streptococcus pneumoniae\n","Answer: Streptococcus pneumoniae (often abbreviated as S. pneumoniae or SP) is a Gram-positive bacterium responsible for severe infections such as meningitis and pneumonia. It's also a major cause of childhood mortality globally, with a significant number of deaths attributed to it each year.  It is also a common cause of nasopharyngeal carriage (meaning it can live in the nose and throat without causing illness), and it exists in over 100 different serotypes (variations).  The development and use of pneumococcal conjugate vaccines (PCVs) have significantly impacted the prevalence of different serotypes and the overall disease burden.\n","Enter your health-related question (or type 'exit'): exit\n"]}]},{"cell_type":"markdown","source":["### MULTIMODAL RAG"],"metadata":{"id":"q15W3jHHToJU"}},{"cell_type":"code","source":["!pip install clip\n","!pip install torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"thE7ZHwNUtkT","executionInfo":{"status":"ok","timestamp":1743661560410,"user_tz":-330,"elapsed":7061,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"0295d7dc-0503-4b5b-8e6a-58c47c8c91ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: clip in /usr/local/lib/python3.11/dist-packages (0.2.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]}]},{"cell_type":"code","source":["import os\n","import gradio as gr\n","import langchain\n","from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain.chains import RetrievalQA\n","from langchain_community.document_loaders import TextLoader, PyPDFLoader\n","from langchain_community.document_loaders import UnstructuredImageLoader\n","import whisper\n","import clip\n","import torch\n","from PIL import Image\n","\n","# Directly set the API key in Colab (less secure, but simpler for Colab)\n","GOOGLE_API_KEY = \"\"  # Replace with your actual API key\n","\n","if not GOOGLE_API_KEY:\n","    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n","\n","# Load models\n","whisper_model = whisper.load_model(\"base\")\n","clip_model, preprocess = clip.load(\"ViT-B/32\")\n","\n","def process_audio(audio_path):\n","    \"\"\"Transcribes audio to text.\"\"\"\n","    result = whisper_model.transcribe(audio_path)\n","    return result[\"text\"]\n","\n","def process_image(image_path):\n","    \"\"\"Generates a text description of an image.\"\"\"\n","    image = preprocess(Image.open(image_path)).unsqueeze(0)\n","    with torch.no_grad():\n","        image_features = clip_model.encode_image(image)\n","    image_features /= image_features.norm(dim=-1, keepdim=True)\n","    return \"This image contains \" + str(clip.tokenize([\"a photo of a \"])[0]) # very basic, needs enhancement.\n","\n","def create_multimodal_chatbot(file_paths):\n","    \"\"\"Creates a multimodal chatbot.\"\"\"\n","    all_texts = []\n","    for file_path in file_paths:\n","        try:\n","            if file_path.lower().endswith(\".pdf\"):\n","                loader = PyPDFLoader(file_path)\n","                documents = loader.load()\n","                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","                texts = text_splitter.split_documents(documents)\n","                all_texts.extend(texts)\n","            elif file_path.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\")):\n","                text_description = process_image(file_path)\n","                all_texts.append(langchain.load.Document(page_content=text_description))\n","            elif file_path.lower().endswith((\".mp3\", \".wav\", \".ogg\")):\n","                transcribed_text = process_audio(file_path)\n","                all_texts.append(langchain.load.Document(page_content=transcribed_text))\n","\n","            else: #default to txt\n","                loader = TextLoader(file_path)\n","                documents = loader.load()\n","                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","                texts = text_splitter.split_documents(documents)\n","                all_texts.extend(texts)\n","\n","        except Exception as e:\n","            print(f\"Error processing {file_path}: {e}\")\n","\n","    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n","    db = FAISS.from_documents(all_texts, embeddings)\n","    retriever = db.as_retriever()\n","    llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n","    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n","    return qa_chain\n","\n","def ask_multimodal_question(question, file_paths, history):\n","    \"\"\"Asks a question and returns the answer with source documents.\"\"\"\n","    qa_chain = create_multimodal_chatbot(file_paths)\n","    if qa_chain:\n","        try:\n","            result = qa_chain({\"query\": question})\n","            answer = result[\"result\"]\n","            source_docs = result[\"source_documents\"]\n","\n","            source_text = \"\\n\\nSource Documents:\\n\"\n","            for doc in source_docs:\n","                source_text += f\"- {doc.page_content[:100]}...\\n\"\n","\n","            return history + [(question, answer + source_text)]\n","\n","        except Exception as e:\n","            return history + [(question, f\"Error: {e}\")]\n","    else:\n","        return history + [(question, \"Chatbot not initialized. Please upload valid files.\")]\n","\n","def main():\n","    \"\"\"Creates and launches the Gradio interface.\"\"\"\n","    with gr.Blocks() as demo:\n","        gr.Markdown(\"# Multimodal Chatbot\")\n","\n","        file_input = gr.Files(label=\"Upload Files (PDF, TXT, Images, Audio)\")\n","        chatbot = gr.Chatbot()\n","        message = gr.Textbox(label=\"Question\")\n","        clear = gr.ClearButton([message, chatbot])\n","\n","        def respond(message, chat_history, file_objs):\n","            if not file_objs:\n","                return \"Please upload files first.\", chat_history\n","            file_paths = [file_obj.name for file_obj in file_objs]\n","            return ask_multimodal_question(message, file_paths, chat_history), chat_history\n","\n","        message.submit(respond, inputs=[message, chatbot, file_input], outputs=[chatbot, chatbot])\n","\n","    demo.launch()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"9VtH8fViTqky","executionInfo":{"status":"error","timestamp":1743661771859,"user_tz":-330,"elapsed":18791,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"ae65fb15-7770-46b4-b2d9-7dabc494abc2"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-28f520c03c44>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnstructuredImageLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/whisper/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2603\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2604\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2606\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   5765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5767\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mregister_decomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5768\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mout_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexact_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5769\u001b[0m def norm(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36mdecomposition_decorator\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# To handle allowing multiple aten_ops at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mpytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_map_\u001b[0;34m(func, tree, is_leaf, *rests)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m     \u001b[0mdeque\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# consume and exhaust the iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregistry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# To handle allowing multiple aten_ops at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m_add_op_to_registry\u001b[0;34m(registry, op, fn)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpOverloadPacket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0moverloads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mop_overload\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverloads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1097\u001b[0m             )\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# cache the overload object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["!pip install Whisper"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TrWEc0y5V6QS","executionInfo":{"status":"ok","timestamp":1743661824851,"user_tz":-330,"elapsed":4752,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"bbc6a7e9-c381-4737-96cb-0ccd7e66d06f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting Whisper\n","  Downloading whisper-1.1.10.tar.gz (42 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from Whisper) (1.17.0)\n","Building wheels for collected packages: Whisper\n","  Building wheel for Whisper (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for Whisper: filename=whisper-1.1.10-py3-none-any.whl size=41120 sha256=7b282573512a3a383fe03b666cf6921bab73cc0d999b390a36ef18f30423e44c\n","  Stored in directory: /root/.cache/pip/wheels/21/65/ee/4e6672aabfa486d3341a39a04f8f87c77e5156149299b5a7d0\n","Successfully built Whisper\n","Installing collected packages: Whisper\n","Successfully installed Whisper-1.1.10\n"]}]},{"cell_type":"code","source":["import os\n","import gradio as gr\n","import langchain\n","from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import FAISS\n","from langchain.chains import RetrievalQA\n","from langchain_community.document_loaders import TextLoader, PyPDFLoader\n","import whisper\n","import clip\n","import torch\n","from PIL import Image\n","\n","# Directly set the API key in Colab (less secure, but simpler for Colab)\n","GOOGLE_API_KEY = \"AIzaSyC_NSrDX__MLPdb9mKqQW12wKAs8Xt68L0\"  # Replace with your actual API key\n","\n","if not GOOGLE_API_KEY:\n","    raise ValueError(\"GOOGLE_API_KEY not set. Please replace 'YOUR_GOOGLE_API_KEY' with your key.\")\n","\n","# Load models\n","whisper_model = whisper.load_model(\"base\")\n","clip_model, preprocess = clip.load(\"ViT-B/32\")\n","\n","def process_audio(audio_path):\n","    \"\"\"Transcribes audio to text.\"\"\"\n","    result = whisper_model.transcribe(audio_path)\n","    return result[\"text\"]\n","\n","def process_image(image_path):\n","    \"\"\"Generates a text description of an image.\"\"\"\n","    image = preprocess(Image.open(image_path)).unsqueeze(0)\n","    with torch.no_grad():\n","        image_features = clip_model.encode_image(image)\n","    image_features /= image_features.norm(dim=-1, keepdim=True)\n","    return \"This image contains \" + str(clip.tokenize([\"a photo of a \"])[0]) # very basic, needs enhancement.\n","\n","def create_multimodal_chatbot(file_paths):\n","    \"\"\"Creates a multimodal chatbot.\"\"\"\n","    all_texts = []\n","    for file_path in file_paths:\n","        try:\n","            if file_path.lower().endswith(\".pdf\"):\n","                loader = PyPDFLoader(file_path)\n","                documents = loader.load()\n","                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","                texts = text_splitter.split_documents(documents)\n","                all_texts.extend(texts)\n","            elif file_path.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".gif\")):\n","                text_description = process_image(file_path)\n","                all_texts.append(langchain.load.Document(page_content=text_description))\n","            elif file_path.lower().endswith((\".mp3\", \".wav\", \".ogg\")):\n","                transcribed_text = process_audio(file_path)\n","                all_texts.append(langchain.load.Document(page_content=transcribed_text))\n","            else: #default to txt\n","                loader = TextLoader(file_path)\n","                documents = loader.load()\n","                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","                texts = text_splitter.split_documents(documents)\n","                all_texts.extend(texts)\n","        except Exception as e:\n","            print(f\"Error processing {file_path}: {e}\")\n","\n","    embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n","    db = FAISS.from_documents(all_texts, embeddings)\n","    retriever = db.as_retriever()\n","    llm = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro\", google_api_key=GOOGLE_API_KEY)\n","    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n","    return qa_chain\n","\n","def ask_multimodal_question(question, file_paths, history):\n","    \"\"\"Asks a question and returns the answer with source documents.\"\"\"\n","    qa_chain = create_multimodal_chatbot(file_paths)\n","    if qa_chain:\n","        try:\n","            result = qa_chain({\"query\": question})\n","            answer = result[\"result\"]\n","            source_docs = result[\"source_documents\"]\n","\n","            source_text = \"\\n\\nSource Documents:\\n\"\n","            for doc in source_docs:\n","                source_text += f\"- {doc.page_content[:100]}...\\n\"\n","\n","            return history + [(question, answer + source_text)]\n","\n","        except Exception as e:\n","            return history + [(question, f\"Error: {e}\")]\n","    else:\n","        return history + [(question, \"Chatbot not initialized. Please upload valid files.\")]\n","\n","def main():\n","    \"\"\"Creates and launches the Gradio interface.\"\"\"\n","    with gr.Blocks() as demo:\n","        gr.Markdown(\"# Multimodal Chatbot\")\n","\n","        file_input = gr.Files(label=\"Upload Files (PDF, TXT, Images, Audio)\")\n","        chatbot = gr.Chatbot()\n","        message = gr.Textbox(label=\"Question\")\n","        clear = gr.ClearButton([message, chatbot])\n","\n","        def respond(message, chat_history, file_objs):\n","            if not file_objs:\n","                return \"Please upload files first.\", chat_history\n","            file_paths = [file_obj.name for file_obj in file_objs]\n","            return ask_multimodal_question(message, file_paths, chat_history), chat_history\n","\n","        message.submit(respond, inputs=[message, chatbot, file_input], outputs=[chatbot, chatbot])\n","\n","    demo.launch()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"_fsAI9M_WcsR","executionInfo":{"status":"error","timestamp":1743662093484,"user_tz":-330,"elapsed":13232,"user":{"displayName":"Thamarai Selvi. R","userId":"14724196935658464059"}},"outputId":"715d9fcc-c6d9-46a2-eca3-59b27a39cac4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'clip' has no attribute 'load'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2ed276d593c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Load models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mwhisper_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-B/32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'load'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JpCqEi2uW9_Z"},"execution_count":null,"outputs":[]}]}
